{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91714,"databundleVersionId":11251744,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T09:28:38.176415Z","iopub.execute_input":"2025-03-20T09:28:38.176633Z","iopub.status.idle":"2025-03-20T09:28:40.888382Z","shell.execute_reply.started":"2025-03-20T09:28:38.176612Z","shell.execute_reply":"2025-03-20T09:28:40.886580Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"file_path = \"/kaggle/input/playground-series-s5e3/train.csv\"\ndf = pd.read_csv(file_path)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:14:18.525735Z","iopub.execute_input":"2025-03-19T10:14:18.526348Z","iopub.status.idle":"2025-03-19T10:14:18.601729Z","shell.execute_reply.started":"2025-03-19T10:14:18.526281Z","shell.execute_reply":"2025-03-19T10:14:18.600815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1Ô∏è‚É£ Check for missing values\nprint(\"Missing values per column:\\n\", df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:14:18.603266Z","iopub.execute_input":"2025-03-19T10:14:18.603619Z","iopub.status.idle":"2025-03-19T10:14:18.612526Z","shell.execute_reply.started":"2025-03-19T10:14:18.603592Z","shell.execute_reply":"2025-03-19T10:14:18.611434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2Ô∏è‚É£ Drop unnecessary columns (e.g., 'id' and 'day' which are not useful for prediction)\ndf = df.drop(columns=[\"id\", \"day\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:14:18.614054Z","iopub.execute_input":"2025-03-19T10:14:18.614440Z","iopub.status.idle":"2025-03-19T10:14:18.626478Z","shell.execute_reply.started":"2025-03-19T10:14:18.614412Z","shell.execute_reply":"2025-03-19T10:14:18.625568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# 4Ô∏è‚É£ Define features (X) and target (y)\nX = df.drop(columns=[\"rainfall\"])  # Features\ny = df[\"rainfall\"]  # Target (binary classification)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:14:18.627416Z","iopub.execute_input":"2025-03-19T10:14:18.627695Z","iopub.status.idle":"2025-03-19T10:14:18.646926Z","shell.execute_reply.started":"2025-03-19T10:14:18.627671Z","shell.execute_reply":"2025-03-19T10:14:18.645827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5Ô∏è‚É£ Split dataset into training (80%) and testing (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:14:18.648006Z","iopub.execute_input":"2025-03-19T10:14:18.648358Z","iopub.status.idle":"2025-03-19T10:14:18.671124Z","shell.execute_reply.started":"2025-03-19T10:14:18.648295Z","shell.execute_reply":"2025-03-19T10:14:18.669359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6Ô∏è‚É£ Feature scaling (Normalization) - Only applied to numeric features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:14:18.674146Z","iopub.execute_input":"2025-03-19T10:14:18.674515Z","iopub.status.idle":"2025-03-19T10:14:18.704238Z","shell.execute_reply.started":"2025-03-19T10:14:18.674485Z","shell.execute_reply":"2025-03-19T10:14:18.703422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ‚úÖ Final output\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train distribution:\\n\", y_train.value_counts())\nprint(\"y_test distribution:\\n\", y_test.value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:14:18.705537Z","iopub.execute_input":"2025-03-19T10:14:18.705835Z","iopub.status.idle":"2025-03-19T10:14:18.719625Z","shell.execute_reply.started":"2025-03-19T10:14:18.705810Z","shell.execute_reply":"2025-03-19T10:14:18.718485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load dataset\nfile_path = \"/kaggle/input/playground-series-s5e3/train.csv\"\ndf = pd.read_csv(file_path)\n\n# Drop unnecessary columns\ndf = df.drop(columns=[\"id\", \"day\"])\n\n# Define features (X) and target (y)\nX = df.drop(columns=[\"rainfall\"])  # Features\ny = df[\"rainfall\"]  # Target variable (0 or 1)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Feature Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# ‚úÖ Logistic Regression Model\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\ny_pred_log = log_reg.predict(X_test)\n\n# ‚úÖ Random Forest Model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_test)\n\n# ‚úÖ XGBoost Model\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\nxgb_model.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_test)\n\n# üéØ Model Evaluation\nprint(\"üìå Logistic Regression Performance:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\nprint(classification_report(y_test, y_pred_log))\n\nprint(\"\\nüìå Random Forest Performance:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(classification_report(y_test, y_pred_rf))\n\nprint(\"\\nüìå XGBoost Performance:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\nprint(classification_report(y_test, y_pred_xgb))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:14:18.720766Z","iopub.execute_input":"2025-03-19T10:14:18.721141Z","iopub.status.idle":"2025-03-19T10:14:19.349626Z","shell.execute_reply.started":"2025-03-19T10:14:18.721095Z","shell.execute_reply":"2025-03-19T10:14:19.348606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ‚úÖ Hyperparameter tuning for Random Forest\nrf_params = {\n    \"n_estimators\": [100, 200, 300],\n    \"max_depth\": [5, 10, 15, None],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4]\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_rf = GridSearchCV(rf, rf_params, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=2)\ngrid_rf.fit(X_train, y_train)\n\n# ‚úÖ Best Random Forest Model\nbest_rf = grid_rf.best_estimator_\ny_pred_rf = best_rf.predict(X_test)\n\nprint(\"\\nüìå Best Random Forest Model:\")\nprint(\"Best Parameters:\", grid_rf.best_params_)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(classification_report(y_test, y_pred_rf))\n\n\n# ‚úÖ Hyperparameter tuning for XGBoost\nxgb_params = {\n    \"n_estimators\": [100, 200, 300],\n    \"max_depth\": [3, 5, 7],\n    \"learning_rate\": [0.01, 0.1, 0.2],\n    \"subsample\": [0.8, 1.0],\n    \"colsample_bytree\": [0.8, 1.0]\n}\n\nxgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\ngrid_xgb = GridSearchCV(xgb, xgb_params, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=2)\ngrid_xgb.fit(X_train, y_train)\n\n# ‚úÖ Best XGBoost Model\nbest_xgb = grid_xgb.best_estimator_\ny_pred_xgb = best_xgb.predict(X_test)\n\nprint(\"\\nüìå Best XGBoost Model:\")\nprint(\"Best Parameters:\", grid_xgb.best_params_)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\nprint(classification_report(y_test, y_pred_xgb))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:14:19.350693Z","iopub.execute_input":"2025-03-19T10:14:19.350969Z","iopub.status.idle":"2025-03-19T10:17:22.167899Z","shell.execute_reply.started":"2025-03-19T10:14:19.350944Z","shell.execute_reply":"2025-03-19T10:17:22.166981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Get feature importance from the best XGBoost model\nfeature_importance = best_xgb.feature_importances_\nfeature_names = X.columns\n\n# Create a DataFrame for visualization\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\nimportance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Importance\", y=\"Feature\", data=importance_df, palette=\"viridis\")\nplt.title(\"Feature Importance (XGBoost)\")\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"Feature\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:22.169165Z","iopub.execute_input":"2025-03-19T10:17:22.169686Z","iopub.status.idle":"2025-03-19T10:17:23.008279Z","shell.execute_reply.started":"2025-03-19T10:17:22.169650Z","shell.execute_reply":"2025-03-19T10:17:23.006912Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The best model is XGBoost with the following results:\n\nAccuracy: 87.44%\nPrecision (Class 1): 0.89, Recall (Class 1): 0.95, F1-Score (Class 1): 0.92\nPrecision (Class 0): 0.80, Recall (Class 0): 0.65, F1-Score (Class 0): 0.72\nWeighted Avg. F1-Score: 0.87","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\n# Train the best XGBoost model with class weights\nbest_xgb_model = XGBClassifier(\n    colsample_bytree=0.8,\n    learning_rate=0.01,\n    max_depth=7,\n    n_estimators=300,\n    subsample=0.8,\n    scale_pos_weight= (len(y_train) - sum(y_train)) / sum(y_train)  # Handle imbalance\n)\n\n# Fit the model\nbest_xgb_model.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:23.009445Z","iopub.execute_input":"2025-03-19T10:17:23.009973Z","iopub.status.idle":"2025-03-19T10:17:23.358600Z","shell.execute_reply.started":"2025-03-19T10:17:23.009941Z","shell.execute_reply":"2025-03-19T10:17:23.357709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid for XGBoost\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [6, 7, 8],\n    'n_estimators': [100, 200, 300],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0],\n}\n\n# Use GridSearchCV for hyperparameter optimization\ngrid_search = GridSearchCV(estimator=XGBClassifier(), param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)\ngrid_search.fit(X_train, y_train)\n\n# Best model from grid search\nbest_xgb_model = grid_search.best_estimator_\nprint(\"Best parameters:\", grid_search.best_params_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:23.359404Z","iopub.execute_input":"2025-03-19T10:17:23.359684Z","iopub.status.idle":"2025-03-19T10:17:58.362970Z","shell.execute_reply.started":"2025-03-19T10:17:23.359659Z","shell.execute_reply":"2025-03-19T10:17:58.361411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\n# Train the model (assuming you have X_train, y_train)\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:21:21.734454Z","iopub.execute_input":"2025-03-19T10:21:21.734778Z","iopub.status.idle":"2025-03-19T10:21:21.838428Z","shell.execute_reply.started":"2025-03-19T10:21:21.734754Z","shell.execute_reply":"2025-03-19T10:21:21.837593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_prob = model.predict_proba(X_test)[:, 1]  # Probability for Class 1 (rain)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:21:29.963645Z","iopub.execute_input":"2025-03-19T10:21:29.963980Z","iopub.status.idle":"2025-03-19T10:21:29.970918Z","shell.execute_reply.started":"2025-03-19T10:21:29.963951Z","shell.execute_reply":"2025-03-19T10:21:29.970125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load train dataset\ntrain_file_path = \"/kaggle/input/playground-series-s5e3/train.csv\"\ndf = pd.read_csv(train_file_path)\n\n# Drop unnecessary columns\ndf = df.drop(columns=[\"id\", \"day\"])\n\n# Define features (X) and target (y)\nX = df.drop(columns=[\"rainfall\"])  # Features\ny = df[\"rainfall\"]  # Target variable (0 or 1)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Feature Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# ‚úÖ Train XGBoost Model\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\nxgb_model.fit(X_train, y_train)\n\n# üéØ Model Evaluation\ny_pred_xgb = xgb_model.predict(X_test)\nprint(\"\\nüìå XGBoost Performance:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\nprint(classification_report(y_test, y_pred_xgb))\n\n# ‚úÖ Load Kaggle Test Dataset\ntest_file_path = \"/kaggle/input/playground-series-s5e3/test.csv\"\ntest_data = pd.read_csv(test_file_path)\n\n# Drop unnecessary columns (assumes same structure as train)\ntest_ids = test_data[\"id\"]  # Save 'id' for submission\ntest_data = test_data.drop(columns=[\"id\", \"day\"])\n\n# Scale test data\nX_kaggle_test = scaler.transform(test_data)\n\n# Predict probabilities for Kaggle test set\ny_prob = xgb_model.predict_proba(X_kaggle_test)[:, 1]  # Probability of rainfall (Class 1)\n\n# ‚úÖ Create Kaggle Submission File\nsubmission = pd.DataFrame({\n    \"id\": test_ids,\n    \"Probability\": y_prob\n})\n\n# Save as CSV\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"\\n‚úÖ Submission file saved as 'submission.csv'. Ready for Kaggle upload!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:22:48.859194Z","iopub.execute_input":"2025-03-19T10:22:48.859581Z","iopub.status.idle":"2025-03-19T10:22:49.009799Z","shell.execute_reply.started":"2025-03-19T10:22:48.859554Z","shell.execute_reply":"2025-03-19T10:22:49.008075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}